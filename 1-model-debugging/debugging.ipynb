{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d543406-aefb-4ad5-ba7c-eb75e5fa08f5",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "Author: **Christian Lessig et al.**\n",
    "\n",
    "`christian.lessig@ecmwf.int`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e9100",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Most often, we spend more time debugging code than writing it. This is particularly true for python.\n",
    "\n",
    "Debugging usually consists of three steps:\n",
    "1. Localize the problem.\n",
    "2. Understand what precisely goes wrong.\n",
    "3. Fix the problem.\n",
    "The third step is usually the easy one once the first two have been accomplished.\n",
    "\n",
    "To localize the problem and understand the issue, it is often important to have an understanding of the software stack that is used to run your code. Many error messages will result from somewhere in the stack and not directly from the user code.\n",
    "\n",
    "<img src=\"ml_stack.png\" width=\"400px\" >\n",
    "\n",
    "In simple cases when execution breaks, localizing the problem means to parse the error messages and map it to the code and the call stack. The problem might very well originate elsewhere but where the code breaks is the entry point for you to localize and understand the root cause.\n",
    "\n",
    "Ones an entry point into the problem has been found, one can investigate what goes wrong. This means almost always to set a break point before the offending line and investigate the state of the program and the code. Simple typos might not require this but in all other circumstances it is easier to use a breakpoint. In python one can break with:\n",
    "\n",
    "```\n",
    "import pdb; pdb.set_trace()\n",
    "```\n",
    "\n",
    "This opens a debugger shell in the code line following the one where the statement is. Alternatively, one can use:\n",
    "\n",
    "```\n",
    "code.interact( local=locals())\n",
    "```\n",
    "\n",
    "This opens an interactive python shell in the calling line but does not provide the functionality of a debugger (e.g. a stack trace). However, it can be useful for quick inspection or or code development.\n",
    "\n",
    "The common cause for bugs is that an assumption about the input/output data is violated. This can be the shape of a tensor (easy) or unexpected values (difficult) or something more subtle (very difficult). In the interactive debugger shell you can investigate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04366dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "import code\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a94abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "reload( model)\n",
    "from model import MLP\n",
    "\n",
    "net = MLP( dim_in=512, dim_out=512)\n",
    "\n",
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab76a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if we can evaluate the network\n",
    "\n",
    "t_in = torch.rand( (16, 4, 512)).to(device)\n",
    "t_out = net( t_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a252dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if data loading works\n",
    "\n",
    "import dataset\n",
    "reload( dataset)\n",
    "from dataset import CustomDataSet\n",
    "\n",
    "custom_dataset = CustomDataSet( len=32768, batch_size=128, dim_data=512)\n",
    "data_iter = iter(custom_dataset)\n",
    "\n",
    "lossfct = torch.nn.MSELoss()\n",
    "\n",
    "# load sample\n",
    "(source, target) = next(data_iter)\n",
    "source, target = source.to(device), target.to(device)\n",
    "\n",
    "# evaluate network\n",
    "pred = net( source)\n",
    "\n",
    "# compute loss\n",
    "loss = lossfct( pred, target)\n",
    "\n",
    "print( f'loss : {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1023113-3009-4c05-9e6f-983b23e96978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "reload( dataset)\n",
    "from dataset import CustomDataSet\n",
    "\n",
    "# training loop\n",
    "\n",
    "optimizer = torch.optim.AdamW( net.parameters(), lr=0.00005)\n",
    "\n",
    "# parallel data loader\n",
    "loader_params = { 'batch_size': None, 'batch_sampler': None, 'shuffle': False, \n",
    "                   'num_workers': 8, 'pin_memory': True}\n",
    "dataloader = torch.utils.data.DataLoader( custom_dataset, **loader_params, sampler = None)\n",
    "\n",
    "num_epochs = 8\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for epoch in range( num_epochs) :\n",
    "  \n",
    "  data_iter = iter( dataloader)\n",
    "  \n",
    "  for bidx, (source, target) in enumerate(data_iter) :\n",
    "\n",
    "    source, target = source.to(device), target.to(device)\n",
    "    \n",
    "    pred = net( source)\n",
    "    loss = lossfct( pred, target)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  print( f'Finished epoch={epoch} with loss={loss}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.arange( 512)\n",
    "loss = lossfct( source[idx], target[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
